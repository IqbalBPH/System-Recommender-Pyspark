# -*- coding: utf-8 -*-
"""Pyspark_System_Recommender_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17m5LcxdzdHMcNDTyH1SJHjWO15LXiUn_
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.ml.feature import StringIndexer, VectorAssembler, HashingTF, IDF, Tokenizer, StopWordsRemover
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline
from pyspark.ml.clustering import KMeans
from pyspark.mllib.linalg.distributed import RowMatrix
from pyspark.ml.linalg import Vectors, VectorUDT
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql import functions as F  # Menggunakan alias untuk menghindari konflik
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
from pyspark.sql.window import Window
from pyspark.sql.functions import col,isnan, when, count, mean,  split, explode, isnull, trim, desc, concat_ws, lower, regexp_replace, repeat
from pyspark.sql import DataFrame
from typing import Optional, List
from pyspark.ml.linalg import Vectors, DenseVector, SparseVector
from pyspark.sql.functions import udf
from pyspark.broadcast import Broadcast
from pyspark.ml.feature import RegexTokenizer, NGram, Normalizer
warnings.filterwarnings('ignore')
print("🚀 IMPORT COMPLETED!")

# Inisialisasi Spark Session
spark = SparkSession.builder \
    .appName("Amazon Recommender System") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

# Set log level untuk mengurangi noise
spark.sparkContext.setLogLevel("WARN")

print("🚀 Spark Session berhasil dibuat!")
print(f"Spark Version: {spark.version}")
print(f"Available Cores: {spark.sparkContext.defaultParallelism}")

# Membaca data CSV dengan error handling yang lebih baik
try:
    df = spark.read.option("header", "true") \
        .option("inferSchema", "true") \
        .option("multiline", "true") \
        .option("escape", '"') \
        .option("spark.sql.shuffle.partitions", 20) \
        .option("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .option("quote", '"') \
        .csv("amazon.csv")


    print(f"✅ Data berhasil dimuat: {df.count()} baris")
except Exception as e:
    print(f"❌ Error saat membaca file: {e}")
    # Coba dengan opsi yang lebih sederhana
    df = spark.read.option("header", "true").csv("amazon.csv")
    print("✅ Data dimuat dengan opsi sederhana")

"""📊 **INFORMASI DASAR DATASET**"""

print("\n📊 INFORMASI DASAR DATASET")
print("=" * 50)
print(f"Jumlah Baris: {df.count()}")
print(f"Jumlah Kolom: {len(df.columns)}")

# Cek kolom yang tersedia
print("\n📋 KOLOM YANG TERSEDIA:")
for i, column in enumerate(df.columns, 1):
    print(f"{i:2d}. {column}")

# Menampilkan schema dengan penanganan yang lebih baik
print("\n🔍 SCHEMA DATASET")
print("=" * 50)
df.printSchema()

# Tampilkan sample data
print("\n📋 SAMPLE DATA (3 baris pertama)")
print("=" * 50)
df.show(3, truncate=True)

"""📊 **CLEANING DATASET**

Missing Value
"""

def check_missing_values(df: DataFrame,
                        columns: Optional[List[str]] = None,) -> Optional[DataFrame]:

    # Tentukan kolom yang akan dicek
    if columns is None:
        columns_to_check = df.columns
    else:
        # Validasi kolom yang diminta ada di DataFrame
        invalid_cols = [col for col in columns if col not in df.columns]
        if invalid_cols:
            raise ValueError(f"Kolom tidak ditemukan: {invalid_cols}")
        columns_to_check = columns

    # expression untuk menghitung missing values
    missing_expressions = [count(when(isnan(col(c)) | isnull(col(c)), c)).alias(c) for c in columns_to_check]

    # Hitung missing values
    result_df = df.select(missing_expressions).show()

    return result_df

# Cek null value untuk setiap kolom
check_missing_values(df)

# Kita ganti null values di rating count dengan nilai rata rata rating_count

# Hitung mean rating_count
mean_value = df.select(mean(col("rating_count"))).collect()[0][0]

# Ganti nilai null dengan mean
df = df.fillna({"rating_count": mean_value})

# Cek lagi null value untuk setiap kolom
check_missing_values(df)

"""DUPLICATE DATA"""

# 2. Deteksi dan tangani duplikat
def analisis_duplikat(df, key_columns=None):
    if key_columns is None:
        key_columns = ["product_id"]

    # Hitung jumlah kemunculan untuk setiap nilai unik pada key_columns
    window = Window.partitionBy(key_columns)
    df_with_count = df.withColumn("duplicate_count", F.count("*").over(window))

    # Identifikasi duplikat (duplicate_count > 1)
    duplicate_counts = df_with_count.filter(F.col("duplicate_count") > 1) \
                                   .select(*key_columns, "duplicate_count") \
                                   .distinct()

    num_unique = df.select(*key_columns).distinct().count()
    num_duplicates = df.count() - num_unique

    print(f"\nANALISIS DUPLIKAT BERDASARKAN {', '.join(key_columns)}:")
    print(f"Total baris: {df.count()}")
    print(f"Baris unik: {num_unique}")
    print(f"Baris duplikat: {num_duplicates}")

    if duplicate_counts.count() > 0:
        print("\nContoh data duplikat:")
        duplicate_counts.show(3, truncate=False)
    else:
        print("Tidak ditemukan data duplikat.")

    return num_duplicates

# Hapus duplikat berdasarkan product_id
def hapus_duplikat(df, key_columns=None):
    if key_columns is None:
        key_columns = ["product_id"]

    # Drop duplikat, simpan kolom rating tertinggi
    df_no_dupes_higher_rating = df.orderBy(F.desc("rating")).dropDuplicates(key_columns)
    return df_no_dupes_higher_rating

# Analisis duplikat sebelum pembersihan
analisis_duplikat(df, ["product_id"])

# Hapus duplikat
df = hapus_duplikat(df, ["product_id"])

# Analisis duplikat setelah pembersihan
analisis_duplikat(df, ["product_id"])

"""**DATA ENRICHMENT (TERUTAMA MEMBANTU VISUALISASI)**"""

# PEMBERSIHAN DATA DENGAN METODE YANG LEBIH ROBUST
print("\n🧹 PROSES PEMBERSIHAN DATA")
print("=" * 50)

# Menggunakan F.col() instead of col() untuk menghindari konflik
try:
    # Membersihkan kolom harga dengan regex yang lebih spesifik
    df_clean = df.withColumn("discounted_price_clean",
                            F.regexp_replace(F.col("discounted_price"), "[₹,]", "")) \
                 .withColumn("actual_price_clean",
                            F.regexp_replace(F.col("actual_price"), "[₹,]", "")) \
                 .withColumn("discount_percentage_clean",
                            F.regexp_replace(F.col("discount_percentage"), "%", ""))

    print("✅ Pembersihan teks berhasil")

    # Konversi ke numeric dengan error handling
    df_clean = df_clean.withColumn("discounted_price_numeric",
                                  F.col("discounted_price_clean").cast("double")) \
                       .withColumn("actual_price_numeric",
                                  F.col("actual_price_clean").cast("double")) \
                       .withColumn("discount_percentage_numeric",
                                  F.col("discount_percentage_clean").cast("double")) \
                       .withColumn("rating_numeric",
                                  F.col("rating").cast("double"))

    print("✅ Konversi numeric berhasil")
    print("Jumlah Data setelah pembersihan :", df_clean.count())

except Exception as e:
    print(f"❌ Error dalam pembersihan data: {e}")
    print("🔄 Mencoba metode alternatif...")

    # Metode alternatif jika regex gagal
    df_clean = df.select("*")
    for column in ["discounted_price", "actual_price"]:
        if column in df.columns:
            df_clean = df_clean.withColumn(f"{column}_numeric",
                                         F.regexp_replace(F.col(column), "[^0-9.]", "").cast("double"))

# Cek hasil pembersihan data
print("\n🔍 CEK HASIL PEMBERSIHAN")
print("=" * 50)

# Tampilkan kolom yang berhasil dibuat
numeric_columns = [col for col in df_clean.columns if "numeric" in col]
print(f"Kolom numeric yang berhasil dibuat: {numeric_columns}")

# Statistik deskriptif untuk kolom yang berhasil
if numeric_columns:
    try:
        print("\n📊 STATISTIK DESKRIPTIF")
        df_clean.select(*numeric_columns).describe().show()
    except Exception as e:
        print(f"Error dalam statistik: {e}")

"""**OUTLIER,**

outlier tidak dibersihkan karena Dalam Konteks ALS Collaborative Filtering:
- Rating ekstrem justru membantu model mempelajari preferensi pengguna.
- Dalam sistem rekomendasi, variasi rating penting untuk melatih model.

**EXPLORASI DATA (EDA)**
"""

# INSIGHT DAN REKOMENDASI
print("\n💡 RINGKASAN ANALISIS")
print("=" * 50)

try:
    total_products = df.count()
    valid_price_products = df_clean.filter(F.col("discounted_price_numeric").isNotNull()).count()
    valid_rating_products = df_clean.filter(F.col("rating").isNotNull()).count()

    print(f"📊 Total produk: {total_products}")
    print(f"📊 Produk dengan harga valid: {valid_price_products}")
    print(f"📊 Produk dengan rating valid: {valid_rating_products}")

    # Statistik basic jika ada data valid
    if valid_price_products > 0:
        price_stats = df_clean.filter(F.col("discounted_price_numeric").isNotNull()) \
                             .agg(F.min("discounted_price_numeric").alias("min_price"),
                                  F.max("discounted_price_numeric").alias("max_price"),
                                  F.avg("discounted_price_numeric").alias("avg_price")) \
                             .collect()[0]

        print(f"💰 Rentang harga: ₹{price_stats['min_price']:.0f} - ₹{price_stats['max_price']:.0f}")
        print(f"💰 Rata-rata harga: ₹{price_stats['avg_price']:.0f}")

    if valid_rating_products > 0:
        rating_stats = df_clean.filter(F.col("rating_numeric").isNotNull()) \
                              .agg(F.avg("rating_numeric").alias("avg_rating")) \
                              .collect()[0]['avg_rating']

        print(f"⭐ Rata-rata rating: {rating_stats:.2f}")

except Exception as e:
    print(f"Error dalam ringkasan: {e}")

# TOP 10 KATEGORI
print("\n🏷️ ANALISIS KATEGORI PRODUK")
print("=" * 50)

if "category" in df.columns:
    try:
        category_analysis = df_clean.groupBy("category") \
                                   .count() \
                                   .orderBy(F.desc("count"))

        print("Top 10 Kategori:")
        category_analysis.show(10, truncate=False)

        # Simpan untuk visualisasi
        category_pandas = category_analysis.limit(10).toPandas()

    except Exception as e:
        print(f"Error dalam analisis kategori: {e}")

# ANALISIS RATING
print("\n⭐ ANALISIS RATING")
print("=" * 50)

if "rating" in df.columns:
    try:
        rating_analysis = df_clean.filter(F.col("rating").isNotNull()) \
                                 .groupBy("rating") \
                                 .count() \
                                 .orderBy("rating")

        rating_analysis.show()

        # Konversi untuk visualisasi
        rating_pandas = rating_analysis.toPandas()

    except Exception as e:
        print(f"Error dalam analisis rating: {e}")

print("\n📊 PERSIAPAN VISUALISASI")
print("=" * 50)

try:
    # Filter data yang valid untuk visualisasi
    valid_data = df_clean.filter(F.col("discounted_price_numeric").isNotNull() &
                                F.col("rating_numeric").isNotNull())

    # Konversi ke pandas dengan sampel jika data terlalu besar
    if valid_data.count() > 1000:
        sample_data = valid_data.sample(0.8, seed=42)  # Ambil 80% sample
        pdf = sample_data.toPandas()
        print(f"✅ Menggunakan sample data: {len(pdf)} records")
    else:
        pdf = valid_data.toPandas()
        print(f"✅ Menggunakan full data: {len(pdf)} records")

    # VISUALISASI DENGAN PLOTLY
    print("\n🎨 MEMBUAT VISUALISASI")
    print("=" * 50)

    # 1. Distribusi Rating
    if 'rating_numeric' in pdf.columns and not pdf['rating_numeric'].isna().all():
        fig2 = px.histogram(pdf, x='rating_numeric',
                           title='⭐ Distribusi Rating Produk',
                           labels={'rating_numeric': 'Rating', 'count': 'Jumlah Produk'},
                           nbins=10)
        fig2.show()
        print("✅ Histogram rating berhasil dibuat")

    # 2. Scatter plot Harga vs Rating
    if all(col in pdf.columns for col in ['discounted_price_numeric', 'rating_numeric']):
        # Filter outlier untuk visualisasi yang lebih baik
        pdf_filtered = pdf[(pdf['discounted_price_numeric'] < pdf['discounted_price_numeric'].quantile(0.95))]

        fig3 = px.scatter(pdf_filtered,
                         x='discounted_price_numeric',
                         y='rating_numeric',
                         title='💰 Hubungan Harga vs Rating',
                         labels={'discounted_price_numeric': 'Harga (₹)',
                                'rating_numeric': 'Rating'},
                         opacity=0.7)
        fig3.show()
        print("✅ Scatter plot berhasil dibuat")

except Exception as e:
    print(f"❌ Error dalam visualisasi: {e}")
    print("💡 Tip: Pastikan data memiliki nilai numeric yang valid")

# ANALISIS LANJUTAN produk terbaik berdasarkan harga dan rating.
def analyze_top_products(df, price_col="discounted_price_numeric", rating_col="rating_numeric", limit=5):
    try:
        # Produk dengan rating tertinggi
        top_rated = df_clean.filter(F.col(rating_col).isNotNull()) \
                     .orderBy(F.desc(rating_col)) \
                     .select("product_name", price_col, rating_col) \
                     .limit(limit)

        print(f"\n🏆 TOP {limit} PRODUK RATING TERTINGGI:")
        top_rated.show(limit, truncate=False)

        # Produk termurah dengan rating bagus (≥4.0)
        best_value = df_clean.filter((F.col(rating_col) >= 4.0) &
                              (F.col(price_col).isNotNull())) \
                       .orderBy(F.asc(price_col)) \
                       .select("product_name", price_col, rating_col) \
                       .limit(limit)

        print(f"\n💎 TOP {limit} PRODUK VALUE TERBAIK (Rating ≥4.0, Harga Termurah):")
        best_value.show(limit, truncate=False)

        return top_rated, best_value

    except Exception as e:
        print(f"Error dalam analisis top products: {e}")
        return None, None

# Jalankan analisis top products
if 'discounted_price_numeric' in df_clean.columns and 'rating_numeric' in df_clean.columns:
    top_rated, best_value = analyze_top_products(df_clean)

"""**PRE-PROCESSING**"""

# Preprocessing untuk User-Item Data
# Explode user_id yang berformat comma-separated menjadi individual rows
# Alasan: Data user_id dalam format string terpisah koma, perlu di-split untuk individual interactions

# Split user_id dan buat individual user-product interactions
df_interactions = df_clean.select(
    explode(split(col("user_id"), ",")).alias("user_id_clean"),
    col("product_id"),
    col("rating"),
    col("category"),
    col("product_name"),
    col("about_product")
).filter(
    # Filter out empty user IDs
    trim(col("user_id_clean")) != ""
)

# Remove duplicates - ambil rating tertinggi jika ada duplikat user-product
# df_interactions = df_interactions.groupBy("user_id_clean", "product_id").agg(
#     max("rating").alias("rating"),
#     first("category").alias("category"),
#     first("product_name").alias("product_name"),
#     first("about_product").alias("about_product")
# )

df_interactions.cache()
print(f"Individual interactions: {df_interactions.count()} rows")
df_interactions.show(3)

# Analisis duplikat
analisis_duplikat(df_interactions, ["user_id_clean", "product_id"])

# Hapus duplikat
df_interactions = hapus_duplikat(df_interactions, ["user_id_clean", "product_id"])

# Analisis duplikat
analisis_duplikat(df_interactions, ["user_id_clean", "product_id"])

"""**📊 TRANSFORMING DATA**"""

# Convert string IDs to numeric untuk ALS algorithm
# Alasan: ALS membutuhkan numeric IDs untuk efisiensi computational

# Create indexers untuk user dan product
user_indexer = StringIndexer(
    inputCol="user_id_clean",
    outputCol="user_id_numeric",
    handleInvalid="skip"  # Skip invalid values
)

product_indexer = StringIndexer(
    inputCol="product_id",
    outputCol="product_id_numeric",
    handleInvalid="skip"
)

# Fit dan transform data
user_model = user_indexer.fit(df_interactions)
df_indexed = user_model.transform(df_interactions)

product_model = product_indexer.fit(df_indexed)
df_indexed = product_model.transform(df_indexed)

# Select final columns untuk modeling
df_final = df_indexed.select(
    col("user_id_numeric").cast("int").alias("user_id"),
    col("product_id_numeric").cast("int").alias("product_id"),
    col("rating").cast("float"),
    col("user_id_clean").alias("original_user_id"),
    col("product_name"),
    col("category"),
    col("about_product")
)

df_final.cache()
print(f"Final dataset for modeling: {df_final.count()} rows")
df_final.show(5)

check_missing_values(df_final)

# Kita ganti null values di rating count dengan nilai rata rata rating_count

# Hitung mean rating_count
mean_value = df_final.select(mean(col("rating"))).collect()[0][0]

# Ganti nilai null dengan mean
df_final = df_final.fillna({"rating": mean_value})

check_missing_values(df_final)

# Visualization dengan matplotlib (convert ke Pandas untuk plotting)
# Alasan: Spark tidak memiliki built-in plotting, convert ke Pandas untuk visualization

print("Creating visualizations...")

# Rating distribution
rating_dist_pd = df_final.groupBy("rating").count().toPandas()

plt.figure(figsize=(15, 10))

# Plot 1: Rating Distribution
plt.subplot(2, 3, 1)
plt.bar(rating_dist_pd['rating'], rating_dist_pd['count'], color='skyblue')
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Count')

# Plot 2: Category Distribution (Top 10)
category_dist = df_final.groupBy("category").count().orderBy(desc("count")).limit(10).toPandas()
plt.subplot(2, 3, 2)
plt.barh(range(len(category_dist)), category_dist['count'])
plt.yticks(range(len(category_dist)), [cat[:30] + '...' if len(cat) > 30 else cat for cat in category_dist['category']])
plt.title('Top 10 Categories')
plt.xlabel('Count')

# Plot 4: User Activity Distribution
user_activity = df_final.groupBy("user_id").count().select("count").toPandas()
plt.subplot(2, 3, 4)
plt.hist(user_activity['count'], bins=50, color='lightgreen')
plt.title('User Activity Distribution')
plt.xlabel('Number of Ratings per User')
plt.ylabel('Frequency')

# Plot 5: Product Popularity
product_popularity = df_final.groupBy("product_id").count().select("count").toPandas()
plt.subplot(2, 3, 5)
plt.hist(product_popularity['count'], bins=50, color='orange')
plt.title('Product Popularity Distribution')
plt.xlabel('Number of Ratings per Product')
plt.ylabel('Frequency')


plt.tight_layout()
plt.show()

"""**MODELING**"""

(training_data, test_data) = df_final.randomSplit([0.85, 0.15], seed=42)

training_data.cache()
test_data.cache()

print(f"Training data: {training_data.count()} rows")
print(f"Test data: {test_data.count()} rows")

# Cek distribusi rating
print("\nTraining data rating distribution:")
training_data.groupBy("rating").count().orderBy("rating").show(5)

print("\nTest data rating distribution:")
test_data.groupBy("rating").count().orderBy("rating").show(5)

"""**ALS METHOD**"""

# Collaborative Filtering dengan ALS (Alternating Least Squares)
# Alasan menggunakan ALS: Algoritma matrix factorization yang scalable dan efektif untuk CF

# Configure ALS parameters
als = ALS(
    maxIter=10,              # Maximum iterations
    regParam=0.1,            # Regularization parameter
    rank=100,                 # Number of latent factors
    userCol="user_id",
    itemCol="product_id",
    ratingCol="rating",
    coldStartStrategy="drop", # Handle cold start problem
    nonnegative=True,        # Non-negative matrix factorization
    seed=42
)

print("Training ALS model...")
als_model = als.fit(training_data)
print("ALS model training completed!")

# Model Evaluation
# Generate predictions untuk test data
predictions = als_model.transform(test_data)

# Remove NaN predictions (cold start cases)
predictions_clean = predictions.filter(col("prediction").isNotNull())

# Calculate RMSE (Root Mean Square Error)
evaluator = RegressionEvaluator(
    metricName="rmse",
    labelCol="rating",
    predictionCol="prediction"
)

rmse = evaluator.evaluate(predictions_clean)
print(f"Root-mean-square error (RMSE): {rmse:.4f}")

# Calculate MAE (Mean Absolute Error)
evaluator_mae = RegressionEvaluator(
    metricName="mae",
    labelCol="rating",
    predictionCol="prediction"
)

mae = evaluator_mae.evaluate(predictions_clean)
print(f"Mean Absolute Error (MAE): {mae:.4f}")

# Show sample predictions
print("\nSample Predictions:")
predictions_clean.select("user_id", "product_id", "rating", "prediction").show(10)

"""**Content-Based Similarity METHOD**"""

# Content-Based Filtering menggunakan TF-IDF
# Alasan: Menggunakan product features untuk recommendation berdasarkan similarity

print("Building Content-Based Filtering...")

# Prepare product features untuk content-based filtering
products_df = df_final.select(
    "product_id",
    "product_name",
    "category",
    "about_product",
    "category" # Include category column
).distinct()

products_df.show(5)

# Combine text features
products_df = products_df.withColumn(
        "combined_features",
        lower(
            regexp_replace(
                trim(concat_ws(" ",
                    # Weighted features berdasarkan importance
                    repeat(col("product_name"), 3),  # 3x weight untuk nama
                    repeat(col("category"), 2),      # 2x weight untuk kategori
                    col("about_product")
                )),
                "[^a-zA-Z0-9\\s]", " "  # Remove special characters
            )
        )
)

products_df.show(2, truncate=False)
#show only combined_features coloumn
products_df.select("combined_features").show(2, truncate=False)

# Tokenization
tokenizer = RegexTokenizer(
        inputCol="combined_features",
        outputCol="words",
        pattern="\\s+",  # Split on whitespace
        minTokenLength=2  # Filter token pendek
    )
products_tokenized = tokenizer.transform(products_df)

#show only combined_features coloumn
products_tokenized.select("words").show(2, truncate=False)

remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
products_filtered = remover.transform(products_tokenized)

products_filtered.select("filtered_words").show(2, truncate=False)

# TF-IDF Vectorization
hashingTF = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=20000)
products_tf = hashingTF.transform(products_filtered)

products_tf.cache()

products_tf.select("raw_features").show(2, truncate=False)

idf = IDF(inputCol="raw_features", outputCol="tfidf_features", minDocFreq=3)
idf_model = idf.fit(products_tf)
products_tfidf = idf_model.transform(products_tf)

products_tfidf.cache()

products_tfidf.select("tfidf_features").show(2, truncate=False)

normalizer = Normalizer(inputCol="tfidf_features", outputCol="normalized_features")
products_tfidf = normalizer.transform(products_tfidf)

products_tfidf.show(2)

"""**Content-Based Similarity MODELING**"""

def cosine_similarity_matrix(products_tfidf):

    print("=== COSINE SIMILARITY ===\n")

    features_collected = products_tfidf.select(
        "product_id", "product_name", "category", "normalized_features"
    ).collect()

    similarity_results = []

    for i, row_i in enumerate(features_collected):
        similarities = []

        for j, row_j in enumerate(features_collected):
            if i != j:  # Skip self-comparison
                # Compute cosine similarity
                vec1 = row_i["normalized_features"]
                vec2 = row_j["normalized_features"]

                dot_product = float(vec1.dot(vec2))
                norm1 = float(vec1.norm(2))
                norm2 = float(vec2.norm(2))

                if norm1 > 0 and norm2 > 0:
                    similarity = dot_product / (norm1 * norm2)
                else:
                    similarity = 0.0

                similarities.append({
                    "source_product_id": row_i["product_id"],
                    "source_product_name": row_i["product_name"],
                    "target_product_id": row_j["product_id"],
                    "target_product_name": row_j["product_name"],
                    "target_category": row_j["category"],
                    "similarity": similarity
                })

        similarity_results.extend(similarities)

    return similarity_results

def get_recommendations(product_id, similarity_matrix, num_recommendations=5):
    print("=== GET RECOMMENDATION ===\n")
    # Filter untuk produk target
    recommendations = [
        item for item in similarity_matrix
        if item["source_product_id"] == product_id
    ]

    # Sort by similarity (descending)
    recommendations.sort(key=lambda x: x["similarity"], reverse=True)

    return recommendations[:num_recommendations]

def run_content_filtering(df_fina, limit):
    print("=== CONTENT-BASED FILTERING ===\n")

    # Compute similarity matrix
    similarity_matrix = cosine_similarity_matrix(products_tfidf)
    print("✓ Similarity matrix computed")

    # Test dengan sample products
    sample_products = df_final.select("product_id", "product_name", "category").distinct().limit(limit).collect()

    for product_row in sample_products:
        product_id = product_row["product_id"]
        product_name = product_row["product_name"]
        product_category = product_row["category"]

        print(f"\n=== SIMILAR TO: {product_name} ===")
        print(f"Category: {product_category}")
        print("-" * 60)

        recommendations = get_recommendations(product_id, similarity_matrix, 5)

        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                print(f"{i}. {rec['target_product_name']}")
                print(f"   Category: {rec['target_category']}")
                print(f"   Similarity: {rec['similarity']:.4f}")
                print()
        else:
            print("No recommendations found")

        print("=" * 80)

# Jalankan implementasi yang diperbaiki
run_content_filtering(df_final, 2)