# -*- coding: utf-8 -*-
"""Salinan dari Pyspark_System_Recommender_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10TkWexlrUyao4O-Ui07RqaIfeJBHtWpH
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.ml.feature import StringIndexer, VectorAssembler, HashingTF, IDF, Tokenizer, StopWordsRemover, RegexTokenizer, NGram, Normalizer
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql import functions as F  # Menggunakan alias untuk menghindari konflik
import plotly.express as px
import plotly.graph_objects as go
import warnings
from pyspark.sql.window import Window
from pyspark.sql.functions import col,isnan, when, count, mean,  split, explode, isnull, trim, desc, concat_ws, lower, regexp_replace, repeat, avg, abs, udf
from pyspark.sql import DataFrame
from typing import Optional, List
from pyspark.ml.linalg import Vectors, DenseVector, SparseVector, VectorUDT
warnings.filterwarnings('ignore')
print("üöÄ IMPORT COMPLETED!")

# Inisialisasi Spark Session
spark = SparkSession.builder \
    .appName("Amazon Recommender System") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

# Set log level untuk mengurangi noise
spark.sparkContext.setLogLevel("WARN")

print("üöÄ Spark Session berhasil dibuat!")
print(f"Spark Version: {spark.version}")
print(f"Available Cores: {spark.sparkContext.defaultParallelism}")

# Membaca data CSV dengan error handling yang lebih baik
try:
    df = spark.read.option("header", "true") \
        .option("inferSchema", "true") \
        .option("multiline", "true") \
        .option("escape", '"') \
        .option("spark.sql.shuffle.partitions", 20) \
        .option("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .option("quote", '"') \
        .csv("amazon.csv")


    print(f"‚úÖ Data berhasil dimuat: {df.count()} baris")
except Exception as e:
    print(f"‚ùå Error saat membaca file: {e}")

"""üìä **INFORMASI DASAR DATASET**"""

print("\nüìä INFORMASI DASAR DATASET")
print("=" * 50)
print(f"Jumlah Baris: {df.count()}")
print(f"Jumlah Kolom: {len(df.columns)}")

# Cek kolom yang tersedia
print("\nüìã KOLOM YANG TERSEDIA:")
for i, column in enumerate(df.columns, 1):
    print(f"{i:2d}. {column}")

# Menampilkan schema dengan penanganan yang lebih baik
print("\nüîç SCHEMA DATASET")
print("=" * 50)
df.printSchema()

# Tampilkan sample data
print("\nüìã SAMPLE DATA (3 baris pertama)")
print("=" * 50)
df.show(3, truncate=True)

"""üìä **CLEANING DATASET**

Missing Value
"""

def check_missing_values(df: DataFrame,
                        columns: Optional[List[str]] = None,) -> Optional[DataFrame]:

    # Tentukan kolom yang akan dicek
    if columns is None:
        columns_to_check = df.columns
    else:
        # Validasi kolom yang diminta ada di DataFrame
        invalid_cols = [col for col in columns if col not in df.columns]
        if invalid_cols:
            raise ValueError(f"Kolom tidak ditemukan: {invalid_cols}")
        columns_to_check = columns

    # expression untuk menghitung missing values
    missing_expressions = [count(when(isnan(col(c)) | isnull(col(c)), c)).alias(c) for c in columns_to_check]

    # Hitung missing values
    result_df = df.select(missing_expressions).show()

    return result_df

# Cek null value untuk setiap kolom
check_missing_values(df)

# Kita ganti null values di rating count dengan nilai rata rata rating_count

# Hitung mean rating_count
mean_value = df.select(mean(col("rating_count"))).collect()[0][0]

# Ganti nilai null dengan mean
df = df.fillna({"rating_count": mean_value})

# Cek lagi null value untuk setiap kolom
check_missing_values(df)

"""DUPLICATE DATA"""

# 2. Deteksi dan tangani duplikat
def analisis_duplikat(df, key_columns=None):
    if key_columns is None:
        key_columns = ["product_id"]

    # Hitung jumlah kemunculan untuk setiap nilai unik pada key_columns
    window = Window.partitionBy(key_columns)
    df_with_count = df.withColumn("duplicate_count", F.count("*").over(window))

    # Identifikasi duplikat (duplicate_count > 1)
    duplicate_counts = df_with_count.filter(F.col("duplicate_count") > 1) \
                                   .select(*key_columns, "duplicate_count") \
                                   .distinct()

    num_unique = df.select(*key_columns).distinct().count()
    num_duplicates = df.count() - num_unique

    print(f"\nANALISIS DUPLIKAT BERDASARKAN {', '.join(key_columns)}:")
    print(f"Total baris: {df.count()}")
    print(f"Baris unik: {num_unique}")
    print(f"Baris duplikat: {num_duplicates}")

    if duplicate_counts.count() > 0:
        print("\nContoh data duplikat:")
        duplicate_counts.show(3, truncate=False)
    else:
        print("Tidak ditemukan data duplikat.")

    return num_duplicates

# Hapus duplikat berdasarkan product_id
def hapus_duplikat(df, key_columns=None):
    if key_columns is None:
        key_columns = ["product_id"]

    # Drop duplikat, simpan kolom rating tertinggi
    df_no_dupes_higher_rating = df.orderBy(F.desc("rating")).dropDuplicates(key_columns)
    return df_no_dupes_higher_rating

# Analisis duplikat sebelum pembersihan
analisis_duplikat(df, ["product_id"])

# Hapus duplikat
df = hapus_duplikat(df, ["product_id"])

# Analisis duplikat setelah pembersihan
analisis_duplikat(df, ["product_id"])

"""**DATA ENRICHMENT (TERUTAMA MEMBANTU VISUALISASI)**"""

# PEMBERSIHAN DATA DENGAN METODE YANG LEBIH ROBUST
print("\nüßπ PROSES PEMBERSIHAN DATA")
print("=" * 50)

# Menggunakan F.col() instead of col() untuk menghindari konflik
try:
    # Membersihkan kolom harga dengan regex yang lebih spesifik
    df_clean = df.withColumn("discounted_price_clean",
                            F.regexp_replace(F.col("discounted_price"), "[‚Çπ,]", "")) \
                 .withColumn("actual_price_clean",
                            F.regexp_replace(F.col("actual_price"), "[‚Çπ,]", "")) \
                 .withColumn("discount_percentage_clean",
                            F.regexp_replace(F.col("discount_percentage"), "%", ""))

    print("‚úÖ Pembersihan teks berhasil")

    # Konversi ke numeric dengan error handling
    df_clean = df_clean.withColumn("discounted_price_numeric",
                                  F.col("discounted_price_clean").cast("double")) \
                       .withColumn("actual_price_numeric",
                                  F.col("actual_price_clean").cast("double")) \
                       .withColumn("discount_percentage_numeric",
                                  F.col("discount_percentage_clean").cast("double")) \
                       .withColumn("rating_numeric",
                                  F.col("rating").cast("double"))

    print("‚úÖ Konversi numeric berhasil")
    print("Jumlah Data setelah pembersihan :", df_clean.count())

except Exception as e:
    print(f"‚ùå Error dalam pembersihan data: {e}")
    print("üîÑ Mencoba metode alternatif...")

    # Metode alternatif jika regex gagal
    df_clean = df.select("*")
    for column in ["discounted_price", "actual_price"]:
        if column in df.columns:
            df_clean = df_clean.withColumn(f"{column}_numeric",
                                         F.regexp_replace(F.col(column), "[^0-9.]", "").cast("double"))

# Cek hasil pembersihan data
print("\nüîç CEK HASIL PEMBERSIHAN")
print("=" * 50)

# Tampilkan kolom yang berhasil dibuat
numeric_columns = [col for col in df_clean.columns if "numeric" in col]
print(f"Kolom numeric yang berhasil dibuat: {numeric_columns}")

# Statistik deskriptif untuk kolom yang berhasil
if numeric_columns:
    try:
        print("\nüìä STATISTIK DESKRIPTIF")
        df_clean.select(*numeric_columns).describe().show()
    except Exception as e:
        print(f"Error dalam statistik: {e}")

"""**EXPLORASI DATA (EDA)**"""

# INSIGHT DAN REKOMENDASI
print("\nüí° RINGKASAN ANALISIS")
print("=" * 50)

try:
    total_products = df.count()
    valid_price_products = df_clean.filter(F.col("discounted_price_numeric").isNotNull()).count()
    valid_rating_products = df_clean.filter(F.col("rating").isNotNull()).count()

    print(f"üìä Total produk: {total_products}")
    print(f"üìä Produk dengan harga valid: {valid_price_products}")
    print(f"üìä Produk dengan rating valid: {valid_rating_products}")

    # Statistik basic jika ada data valid
    if valid_price_products > 0:
        price_stats = df_clean.filter(F.col("discounted_price_numeric").isNotNull()) \
                             .agg(F.min("discounted_price_numeric").alias("min_price"),
                                  F.max("discounted_price_numeric").alias("max_price"),
                                  F.avg("discounted_price_numeric").alias("avg_price")) \
                             .collect()[0]

        print(f"üí∞ Rentang harga: ‚Çπ{price_stats['min_price']:.0f} - ‚Çπ{price_stats['max_price']:.0f}")
        print(f"üí∞ Rata-rata harga: ‚Çπ{price_stats['avg_price']:.0f}")

    if valid_rating_products > 0:
        rating_stats = df_clean.filter(F.col("rating_numeric").isNotNull()) \
                              .agg(F.avg("rating_numeric").alias("avg_rating")) \
                              .collect()[0]['avg_rating']

        print(f"‚≠ê Rata-rata rating: {rating_stats:.2f}")

except Exception as e:
    print(f"Error dalam ringkasan: {e}")

# TOP 10 KATEGORI
print("\nüè∑Ô∏è ANALISIS KATEGORI PRODUK")
print("=" * 50)

if "category" in df.columns:
    try:
      top_category = df_clean.groupBy("category").count().orderBy(F.desc("count"))

      top_category.show(10, truncate=False)

      # Visualisasi
      top_cat_pd = top_category.limit(10).toPandas()

      plt.figure(figsize=(10,6))
      plt.barh(top_cat_pd['category'], top_cat_pd['count'], color='coral')
      plt.xlabel("Jumlah Interaksi")
      plt.title("Top 10 Kategori Terpopuler")
      plt.gca().invert_yaxis()
      plt.show()


    except Exception as e:
        print(f"Error dalam analisis kategori: {e}")

print("\nüìä PERSIAPAN VISUALISASI")
print("=" * 50)

try:
    # Filter data yang valid untuk visualisasi
    valid_data = df_clean.filter(F.col("discounted_price_numeric").isNotNull() &
                                F.col("rating_numeric").isNotNull())

    pdf = valid_data.toPandas()

    # VISUALISASI DENGAN PLOTLY
    print("\nüé® MEMBUAT VISUALISASI")
    print("=" * 50)

    # Scatter plot Harga vs Rating
    if all(col in pdf.columns for col in ['discounted_price_numeric', 'rating_numeric']):
        # Filter outlier untuk visualisasi yang lebih baik
        pdf_filtered = pdf[(pdf['discounted_price_numeric'] < pdf['discounted_price_numeric'].quantile(0.95))]

        fig3 = px.scatter(pdf_filtered,
                         x='discounted_price_numeric',
                         y='rating_numeric',
                         title='üí∞ Hubungan Harga vs Rating',
                         labels={'discounted_price_numeric': 'Harga (‚Çπ)',
                                'rating_numeric': 'Rating'},
                         opacity=0.7)
        fig3.show()
        print("‚úÖ Scatter plot berhasil dibuat")

except Exception as e:
    print(f"‚ùå Error dalam visualisasi: {e}")
    print("üí° Tip: Pastikan data memiliki nilai numeric yang valid")

"""**OUTLIER,**

outlier tidak dibersihkan karena Dalam Konteks ALS Collaborative Filtering:
- Rating ekstrem justru membantu model mempelajari preferensi pengguna.
- Dalam sistem rekomendasi, variasi rating penting untuk melatih model.
"""

# ANALISIS LANJUTAN produk terbaik berdasarkan harga dan rating.
def analyze_top_products(df, price_col="discounted_price_numeric", rating_col="rating_numeric", limit=5):
    try:
        # Produk dengan rating tertinggi
        top_rated = df_clean.filter(F.col(rating_col).isNotNull()) \
                     .orderBy(F.desc(rating_col)) \
                     .select("product_name", price_col, rating_col) \
                     .limit(limit)

        print(f"\nüèÜ TOP {limit} PRODUK RATING TERTINGGI:")
        top_rated.show(limit, truncate=False)

        # Produk termurah dengan rating bagus (‚â•4.0)
        best_value = df_clean.filter((F.col(rating_col) >= 4.0) &
                              (F.col(price_col).isNotNull())) \
                       .orderBy(F.asc(price_col)) \
                       .select("product_name", price_col, rating_col) \
                       .limit(limit)

        print(f"\nüíé TOP {limit} PRODUK VALUE TERBAIK (Rating ‚â•4.0, Harga Termurah):")
        best_value.show(limit, truncate=False)

        return top_rated, best_value

    except Exception as e:
        print(f"Error dalam analisis top products: {e}")
        return None, None

# Jalankan analisis top products
if 'discounted_price_numeric' in df_clean.columns and 'rating_numeric' in df_clean.columns:
    top_rated, best_value = analyze_top_products(df_clean)

"""**PRE-PROCESSING**"""

# Preprocessing untuk User-Item Data
# Explode user_id yang berformat comma-separated menjadi individual rows
# Data user_id dalam format string terpisah koma, perlu di-split untuk individual interactions

# Split user_id dan buat individual user-product interactions
df_interactions = df_clean.select(
    explode(split(col("user_id"), ",")).alias("user_id_clean"),
    col("product_id"),
    col("rating"),
    col("category"),
    col("product_name"),
    col("about_product")
).filter(
    # Filter out empty user IDs
    trim(col("user_id_clean")) != ""
)

df_interactions.cache()
print(f"Individual interactions: {df_interactions.count()} rows")
df_interactions.show(3)

# Analisis duplikat
analisis_duplikat(df_interactions, ["user_id_clean", "product_id"])

# Hapus duplikat
df_interactions = hapus_duplikat(df_interactions, ["user_id_clean", "product_id"])

# Analisis duplikat
analisis_duplikat(df_interactions, ["user_id_clean", "product_id"])

"""**üìä TRANSFORMING DATA**"""

# Convert string IDs to numeric untuk ALS algorithm
# Alasan: ALS membutuhkan numeric IDs untuk efisiensi computational

# Create indexers untuk user dan product
user_indexer = StringIndexer(
    inputCol="user_id_clean",
    outputCol="user_id_numeric",
    handleInvalid="skip"  # Skip invalid values
)

product_indexer = StringIndexer(
    inputCol="product_id",
    outputCol="product_id_numeric",
    handleInvalid="skip"
)

# Fit dan transform data
user_model = user_indexer.fit(df_interactions)
df_indexed = user_model.transform(df_interactions)

product_model = product_indexer.fit(df_indexed)
df_indexed = product_model.transform(df_indexed)

# Select final columns untuk modeling
df_final = df_indexed.select(
    col("user_id_numeric").cast("int").alias("user_id"),
    col("product_id_numeric").cast("int").alias("product_id"),
    col("rating").cast("float"),
    col("user_id_clean").alias("original_user_id"),
    col("product_name"),
    col("category"),
    col("about_product")
)

df_final.cache()
print(f"Final dataset for modeling: {df_final.count()} rows")
df_final.show(5)

check_missing_values(df_final)

# Kita ganti null values di rating count dengan nilai rata rata rating_count

# Hitung mean rating_count
mean_value = df_final.select(mean(col("rating"))).collect()[0][0]

# Ganti nilai null dengan mean
df_final = df_final.fillna({"rating": mean_value})

check_missing_values(df_final)

# Top 10 Produk Paling Banyak Dibeli/Dinilai
top_products = df_final.groupBy("product_id", "product_name").count().orderBy(F.desc("count"))

top_products.show(10, truncate=False)

# Visualisasi
top_pd = top_products.limit(10).toPandas()

plt.figure(figsize=(10,6))
plt.barh(top_pd['product_name'], top_pd['count'], color='teal')
plt.xlabel("Jumlah Interaksi")
plt.title("Top 10 Produk Terpopuler")
plt.gca().invert_yaxis()
plt.show()

print("Creating visualizations...")

# Rating distribution
rating_dist_pd = df_final.groupBy("rating").count().toPandas()

plt.figure(figsize=(15, 10))

# Plot 1: Rating Distribution
plt.subplot(2, 3, 1)
plt.bar(rating_dist_pd['rating'], rating_dist_pd['count'], color='skyblue')
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

"""**MODELING**"""

(training_data, test_data) = df_final.randomSplit([0.85, 0.15], seed=42)

training_data.cache()
test_data.cache()

print(f"Training data: {training_data.count()} rows")
print(f"Test data: {test_data.count()} rows")

# Cek distribusi rating
print("\nTraining data rating distribution:")
training_data.groupBy("rating").count().orderBy("rating").show(5)

print("\nTest data rating distribution:")
test_data.groupBy("rating").count().orderBy("rating").show(5)

from pyspark.sql.functions import count

active_users = df_final.groupBy("user_id").agg(count("product_id").alias("interaction_count")) \
    .filter("interaction_count >= 2")

df_filtered = df_final.join(active_users, on="user_id", how="inner")

active_users.show(5)

df_final.count()

df_filtered.count()

"""**ALS METHOD**"""

# Collaborative Filtering dengan ALS (Alternating Least Squares)
# Alasan menggunakan ALS: Algoritma matrix factorization yang scalable dan efektif untuk CF

# Configure ALS parameters
als = ALS(
    maxIter=10,              # Maximum iterations
    regParam=0.1,            # Regularization parameter
    rank=100,                 # Number of latent factors
    userCol="user_id",
    itemCol="product_id",
    ratingCol="rating",
    coldStartStrategy="drop", # Handle cold start problem
    nonnegative=True,        # Non-negative matrix factorization
    seed=42
)

print("Training ALS model...")
als_model = als.fit(training_data)
print("ALS model training completed!")

# Model Evaluation
# Generate predictions untuk test data
predictions = als_model.transform(test_data)

# Remove NaN predictions (cold start cases)
predictions_clean = predictions.filter(col("prediction").isNotNull())

# Calculate RMSE (Root Mean Square Error)
evaluator = RegressionEvaluator(
    metricName="rmse",
    labelCol="rating",
    predictionCol="prediction"
)

rmse = evaluator.evaluate(predictions_clean)
print(f"Root-mean-square error (RMSE): {rmse:.4f}")

# Calculate MAE (Mean Absolute Error)
evaluator_mae = RegressionEvaluator(
    metricName="mae",
    labelCol="rating",
    predictionCol="prediction"
)

mae = evaluator_mae.evaluate(predictions_clean)
print(f"Mean Absolute Error (MAE): {mae:.4f}")

# Show sample predictions
print("\nSample Predictions:")
predictions_clean.select("user_id", "product_id", "rating", "prediction").show(5)

"""**Content-Based Similarity METHOD**"""

# Content-Based Filtering menggunakan TF-IDF
# Menggunakan product features untuk recommendation berdasarkan similarity

print("Building Content-Based Filtering...")

# Prepare product features untuk content-based filtering
products_df = df_final.select(
    "product_id",
    "product_name",
    "category",
    "about_product",
    "category" # Include category column
).distinct()

products_df.show(5)

# Combine text features
products_df = products_df.withColumn(
        "combined_features",
        lower(
            regexp_replace(
                trim(concat_ws(" ",
                    # Weighted features berdasarkan importance
                    repeat(col("product_name"), 3),  # 3x weight untuk nama
                    repeat(col("category"), 2),      # 2x weight untuk kategori
                    col("about_product")
                )),
                "[^a-zA-Z0-9\\s]", " "  # Remove special characters
            )
        )
)

products_df.show(2, truncate=False)
#show only combined_features coloumn
products_df.select("combined_features").show(2, truncate=False)

# Tokenization
tokenizer = RegexTokenizer(
        inputCol="combined_features",
        outputCol="words",
        pattern="\\s+",  # Split on whitespace
        minTokenLength=2  # Filter token pendek
    )
products_tokenized = tokenizer.transform(products_df)

#show only combined_features coloumn
products_tokenized.select("words").show(2, truncate=False)

remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
products_filtered = remover.transform(products_tokenized)

products_filtered.select("filtered_words").show(2, truncate=False)

# TF-IDF Vectorization
hashingTF = HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=20000)
products_tf = hashingTF.transform(products_filtered)

products_tf.cache()

products_tf.select("raw_features").show(2, truncate=False)

idf = IDF(inputCol="raw_features", outputCol="tfidf_features", minDocFreq=3)
idf_model = idf.fit(products_tf)
products_tfidf = idf_model.transform(products_tf)

products_tfidf.cache()

products_tfidf.select("tfidf_features").show(2, truncate=False)

normalizer = Normalizer(inputCol="tfidf_features", outputCol="normalized_features")
products_tfidf = normalizer.transform(products_tfidf)

products_tfidf.show(2)

"""**Content-Based Similarity MODELING**"""

def cosine_similarity_matrix(products_tfidf):

    print("=== COSINE SIMILARITY ===\n")

    features_collected = products_tfidf.select(
        "product_id", "product_name", "category", "normalized_features"
    ).collect()

    similarity_results = []

    for i, row_i in enumerate(features_collected):
        similarities = []

        for j, row_j in enumerate(features_collected):
            if i != j:  # Skip self-comparison
                # Compute cosine similarity
                vec1 = row_i["normalized_features"]
                vec2 = row_j["normalized_features"]

                dot_product = float(vec1.dot(vec2))
                norm1 = float(vec1.norm(2))
                norm2 = float(vec2.norm(2))

                if norm1 > 0 and norm2 > 0:
                    similarity = dot_product / (norm1 * norm2)
                else:
                    similarity = 0.0

                similarities.append({
                    "source_product_id": row_i["product_id"],
                    "source_product_name": row_i["product_name"],
                    "target_product_id": row_j["product_id"],
                    "target_product_name": row_j["product_name"],
                    "target_category": row_j["category"],
                    "similarity": similarity
                })

        similarity_results.extend(similarities)

    return similarity_results

def get_recommendations(product_id, similarity_matrix, num_recommendations=5):
    print("=== GET RECOMMENDATION ===\n")
    # Filter untuk produk target
    recommendations = [
        item for item in similarity_matrix
        if item["source_product_id"] == product_id
    ]

    # Sort by similarity (descending)
    recommendations.sort(key=lambda x: x["similarity"], reverse=True)

    return recommendations[:num_recommendations]

def run_content_filtering(df_fina, limit):
    print("=== CONTENT-BASED FILTERING ===\n")

    # Compute similarity matrix
    similarity_matrix = cosine_similarity_matrix(products_tfidf)
    print("‚úì Similarity matrix computed")

    # Test dengan sample products
    sample_products = df_final.select("product_id", "product_name", "category").distinct().limit(limit).collect()

    for product_row in sample_products:
        product_id = product_row["product_id"]
        product_name = product_row["product_name"]
        product_category = product_row["category"]

        print(f"\n=== SIMILAR TO: {product_name} ===")
        print(f"Category: {product_category}")
        print("-" * 60)

        recommendations = get_recommendations(product_id, similarity_matrix, 5)

        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                print(f"{i}. {rec['target_product_name']}")
                print(f"   Category: {rec['target_category']}")
                print(f"   Similarity: {rec['similarity']:.4f}")
                print()
        else:
            print("No recommendations found")

        print("=" * 80)

# Jalankan implementasi
run_content_filtering(df_final, 2)

"""**HYBRID RECOMMENDATION**"""

def hybrid_recommendations_informative(
    user_id,
    als_model,
    similarity_matrix,
    products_tfidf,
    interactions_df,
    cb_score=0.5,
    cf_score=0.5,
    num_recommendations=5
):
    print(f"\n=== Recommendations for User {user_id} ===")

    # ==== Step 1: User Purchase History ====
    user_history = interactions_df.filter(F.col("user_id") == user_id).select(
        "product_id", "rating"
    ).join(
        products_tfidf.select("product_id", "product_name"),
        on="product_id",
        how="left"
    ).limit(num_recommendations)

    history_count = user_history.count()

    if history_count == 0:
        print("\nUser has no previous purchases or interactions.\n")
    else:
        print("\nUser's Previous Purchases:")
        user_history.select(
            "product_id", "product_name", "rating"
        ).show(truncate=False)

    user_purchased = set(
        row["product_id"] for row in user_history.collect()
    )

    # ==== Step 2: Collaborative Filtering ====
    user_df = spark.createDataFrame([(user_id,)], ["user_id"])

    cf_recommendations = als_model.recommendForUserSubset(
        user_df, num_recommendations * 2
    ).select("recommendations").collect()

    cf_products = []
    if cf_recommendations:
        cf_products = cf_recommendations[0]["recommendations"]

    cf_product_ids = [item["product_id"] for item in cf_products]
    cf_scores = {item["product_id"]: item["rating"] for item in cf_products}

    # ==== Step 3: Content-Based Filtering ====
    cb_scores = {}
    for product_id in cf_product_ids:
        similar_items = [
            item for item in similarity_matrix
            if item["source_product_id"] == product_id
        ]

        for item in similar_items:
            target_id = item["target_product_id"]
            score = item["similarity"]

            cb_scores[target_id] = cb_scores.get(target_id, 0.0) + score

    # ==== Step 4: Combine CF and CB ====
    combined_scores = {}

    for pid, score in cf_scores.items():
        combined_scores[pid] = combined_scores.get(pid, 0) + (score * cf_score)

    for pid, score in cb_scores.items():
        combined_scores[pid] = combined_scores.get(pid, 0) + (score * cb_score)

    # ==== Step 5: Sort & Filter ====
    sorted_recommendations = sorted(
        combined_scores.items(), key=lambda x: x[1], reverse=True
    )

    final_recommendations = [
        (pid, score) for (pid, score) in sorted_recommendations
        if pid not in user_purchased
    ][:num_recommendations]

    # ==== Step 6: Fallback to Popular Items ====
    if not final_recommendations:
        print("\n‚ö†Ô∏è  No recommendations found, fallback to Popular Items.\n")

        popular_products = interactions_df.groupBy("product_id").agg(
            F.count("user_id").alias("popularity")
        ).join(
            products_tfidf.select("product_id", "product_name", "category"),
            on="product_id",
            how="left"
        ).orderBy(F.desc("popularity"))

        popular_list = popular_products.filter(
            ~F.col("product_id").isin(list(user_purchased))
        ).limit(num_recommendations).collect()

        final_recommendations = [
            (row["product_id"], row["popularity"]) for row in popular_list
        ]

    # ==== Step 7: Build Output ====
    output_rows = []
    for (pid, score) in final_recommendations:
        product_info = products_tfidf.filter(
            F.col("product_id") == pid
        ).select("product_name", "category").collect()

        if product_info:
            name = product_info[0]["product_name"]
            category = product_info[0]["category"]
        else:
            name = "Unknown Product"
            category = "Unknown"

        output_rows.append((pid, round(score, 6), name, category))

    output_df = spark.createDataFrame(
        output_rows,
        ["product_id", "cf_cb_score_or_popularity", "product_name", "category"]
    )

    print("\nRecommended Products:")
    output_df.show(truncate=False)

    print("\n" + "=" * 80 + "\n")

    return output_df

# Contoh user_id
test_user_id = 1

# Panggil Hybrid Recommendation Informatif
hybrid_recommendations_informative(
    user_id=test_user_id,
    als_model=als_model,
    similarity_matrix=cosine_similarity_matrix(products_tfidf),
    products_tfidf=products_tfidf,
    interactions_df=df_final,  # df_final berisi user_id, product_id, rating
    cb_score=0.3,
    cf_score=0.7,
    num_recommendations=5
)

# Contoh user_id
test_user_id = 9999

# Panggil Hybrid Recommendation Informatif
hybrid_recommendations_informative(
    user_id=test_user_id,
    als_model=als_model,
    similarity_matrix=cosine_similarity_matrix(products_tfidf),
    products_tfidf=products_tfidf,
    interactions_df=df_final,  # df_final berisi user_id, product_id, rating
    cb_score=0.3,
    cf_score=0.7,
    num_recommendations=5
)

"""**EVALUATION**"""

# Model Performance Analysis
# Analisis lebih mendalam tentang performance model

print("=== MODEL PERFORMANCE ANALYSIS ===")

# User dan Item factors analysis
user_factors = als_model.userFactors
item_factors = als_model.itemFactors

print(f"User factors shape: {user_factors.count()} users x {len(user_factors.first()['features'])} factors")
print(f"Item factors shape: {item_factors.count()} items x {len(item_factors.first()['features'])} factors")

# Rating prediction accuracy by rating level
print("\nPrediction Accuracy by Rating Level:")
predictions_clean.groupBy("rating").agg(
    count("*").alias("count"),
    avg("prediction").alias("avg_prediction"),
    avg(abs(col("rating") - col("prediction"))).alias("avg_error")
).orderBy("rating").show()

# Top recommended products overall
print("\nMost Frequently Recommended Products:")
all_recommendations = als_model.recommendForAllUsers(10)
popular_recs = all_recommendations.select(
    explode("recommendations").alias("rec")
).select(
    col("rec.product_id").alias("product_id")
).groupBy("product_id").count().orderBy(desc("count"))

# Join dengan product names
popular_products = popular_recs.join(
    df_final.select("product_id", "product_name").distinct(),
    "product_id"
).select("product_name", "count").limit(10)

popular_products.show(10, truncate=False)

import matplotlib.pyplot as plt

print("Creating visualizations...")

# === Plot 1: Prediction vs Actual (Seluruh Data) ===
pred_all = predictions_clean.select("rating", "prediction").limit(int(0.1 * predictions_clean.count())).toPandas()

plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
plt.scatter(pred_all['rating'], pred_all['prediction'], alpha=0.3, color='teal')
plt.plot([1, 5], [1, 5], 'r--')
plt.xlabel('Actual Rating')
plt.ylabel('Predicted Rating')
plt.title('Prediction vs Actual (All Data)')
plt.grid(True)

# === Plot 2: Model Performance Metrics ===
plt.subplot(1, 2, 2)
metrics = ['RMSE', 'MAE']
values = [rmse, mae]
plt.bar(metrics, values, color=['darkred', 'steelblue'])
plt.title('Model Performance Metrics')
plt.ylabel('Error Value')
for i, v in enumerate(values):
    plt.text(i, v + 0.01, f"{v:.4f}", ha='center', fontweight='bold')
plt.grid(axis='y', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

# Cleanup dan Stop Spark Session
# Alasan: Important untuk free up resources setelah selesai processing

print("Cleaning up resources...")

# Unpersist cached DataFrames
df_clean.unpersist()
df_interactions.unpersist()
df_final.unpersist()
training_data.unpersist()
test_data.unpersist()
products_tfidf.unpersist()

print("Resources cleaned up successfully")

# Stop Spark session
# spark.stop()
# print("Spark session stopped")